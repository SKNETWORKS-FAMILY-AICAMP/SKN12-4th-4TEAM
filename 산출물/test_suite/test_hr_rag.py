"""
HR ëª¨ë“ˆ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
- ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- ë¦¬ë­ì»¤ ì •í™•ë„ í…ŒìŠ¤íŠ¸
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import time
import json
from typing import List, Dict

# Django í™˜ê²½ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'proj4.settings')

import django
django.setup()

from myapp.source.HR.agents.agent_executor import process_query

class HRRAGTester:
    def __init__(self):
        self.test_cases = [
            # HR ì •ì±… ê´€ë ¨ ì§ˆë¬¸ë“¤
            {"question": "ì—°ì°¨ëŠ” ì–´ë–»ê²Œ ì¨ìš”?", "category": "vacation_policy"},
            {"question": "ë³µì§€ í¬ì¸íŠ¸ëŠ” ì–´ë””ì„œ í™•ì¸í•´?", "category": "welfare"}, 
            {"question": "ì¶œê·¼ ì‹œê°„ì€ ëª‡ ì‹œì¸ê°€ìš”?", "category": "work_hours"},
            {"question": "ì¬íƒê·¼ë¬´ ì‹ ì²­ì€ ì–´ë–»ê²Œ?", "category": "remote_work"},
            {"question": "ì›”ê¸‰ë‚  ì–¸ì œì•¼?", "category": "salary"},
            {"question": "í‡´ì‚¬ ì ˆì°¨ ì•Œë ¤ì¤˜", "category": "resignation"},
            {"question": "ì•¼ê·¼ìˆ˜ë‹¹ ë‚˜ì™€ìš”?", "category": "overtime"},
            {"question": "íšŒì‚¬ ë…¸íŠ¸ë¶ ë°˜ë‚© ì ˆì°¨", "category": "equipment"},
            {"question": "ì‹ ì…ì‚¬ì› êµìœ¡ ì¼ì •", "category": "training"},
            {"question": "ë¶€ì„œ ì´ë™ ì‹ ì²­ ë°©ë²•", "category": "transfer"},
            
            # íšŒì‚¬ ì •ë³´ ê´€ë ¨
            {"question": "ìš°ë¦¬ íšŒì‚¬ ëŒ€í‘œê°€ ëˆ„êµ¬ì•¼?", "category": "company_info"},
            {"question": "íšŒì‚¬ ì£¼ì†Œ ì•Œë ¤ì¤˜", "category": "company_info"},
            {"question": "íšŒì‚¬ ì—°í˜ì´ ê¶ê¸ˆí•´", "category": "company_info"},
            {"question": "ì •ì£¼ì˜ì— ëŒ€í•´ ì•Œë ¤ì¤˜", "category": "historical_figure"},
            
            # ë³µí•© ì§ˆë¬¸
            {"question": "ì—°ì°¨ ì“°ê³  ì‹¶ì€ë° ìŠ¹ì¸ ì ˆì°¨ì™€ ì£¼ì˜ì‚¬í•­ ì•Œë ¤ì¤˜", "category": "complex"},
            {"question": "í‡´ì‚¬ ì‹œ ì¸ìˆ˜ì¸ê³„ì™€ ì¥ë¹„ ë°˜ë‚©ì€ ì–´ë–»ê²Œ?", "category": "complex"},
            
            # ì¼ë°˜ ì§€ì‹ ì§ˆë¬¸ (ì™¸ë¶€ ê²€ìƒ‰ í•„ìš”)
            {"question": "ìµœê·¼ AI ê¸°ìˆ  ë™í–¥ì€?", "category": "external_knowledge"},
            {"question": "2024ë…„ ê²½ì œ ì „ë§", "category": "external_knowledge"},
        ]
        
        self.results = []
    
    def analyze_response_quality(self, question: str, response: str) -> Dict:
        """ì‘ë‹µ í’ˆì§ˆ ë¶„ì„"""
        quality_metrics = {
            "has_content": len(response.strip()) > 0,
            "is_structured": any(marker in response for marker in ["**", "##", "1.", "-", "â€¢"]),
            "has_korean": any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in response),
            "word_count": len(response.split()),
            "contains_error": any(error_word in response.lower() for error_word in ["error", "ì˜¤ë¥˜", "ì‹¤íŒ¨", "exception"]),
            "relevance_score": 0  # ê°„ë‹¨í•œ ê´€ë ¨ì„± ì ìˆ˜
        }
        
        # ê°„ë‹¨í•œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        question_words = set(question.lower().split())
        response_words = set(response.lower().split())
        if question_words:
            quality_metrics["relevance_score"] = len(question_words & response_words) / len(question_words)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ (0-100)
        quality_score = 0
        if quality_metrics["has_content"]: quality_score += 20
        if quality_metrics["is_structured"]: quality_score += 20
        if quality_metrics["has_korean"]: quality_score += 20
        if quality_metrics["word_count"] >= 10: quality_score += 20
        if not quality_metrics["contains_error"]: quality_score += 10
        if quality_metrics["relevance_score"] > 0.1: quality_score += 10
        
        quality_metrics["overall_score"] = quality_score
        
        return quality_metrics
    
    def run_single_test(self, test_case: Dict) -> Dict:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰"""
        start_time = time.time()
        
        print(f"ì§ˆë¬¸: {test_case['question']}")
        
        try:
            # HR Agent í˜¸ì¶œ
            response = process_query(test_case["question"])
            end_time = time.time()
            response_time = round((end_time - start_time) * 1000, 2)
            
            # ì‘ë‹µ í’ˆì§ˆ ë¶„ì„
            quality = self.analyze_response_quality(test_case["question"], response)
            
            result = {
                "question": test_case["question"],
                "category": test_case["category"],
                "response": response,
                "response_time_ms": response_time,
                "quality_metrics": quality,
                "status": "PASS" if quality["overall_score"] >= 60 else "FAIL"
            }
            
            # ê²°ê³¼ ì¶œë ¥
            if quality["overall_score"] >= 80:
                print(f"âœ… ìš°ìˆ˜ - í’ˆì§ˆì ìˆ˜: {quality['overall_score']}/100 ({response_time}ms)")
            elif quality["overall_score"] >= 60:
                print(f"ğŸ‘ ì–‘í˜¸ - í’ˆì§ˆì ìˆ˜: {quality['overall_score']}/100 ({response_time}ms)")
            else:
                print(f"âŒ ë¶€ì¡± - í’ˆì§ˆì ìˆ˜: {quality['overall_score']}/100 ({response_time}ms)")
            
            print(f"   ì‘ë‹µ ê¸¸ì´: {quality['word_count']}ë‹¨ì–´")
            
            return result
            
        except Exception as e:
            end_time = time.time()
            response_time = round((end_time - start_time) * 1000, 2)
            
            result = {
                "question": test_case["question"],
                "category": test_case["category"],
                "response": None,
                "response_time_ms": response_time,
                "quality_metrics": {"overall_score": 0, "contains_error": True},
                "status": "ERROR",
                "error": str(e)
            }
            
            print(f"ğŸš¨ ì˜ˆì™¸ ë°œìƒ - {str(e)} ({response_time}ms)")
            return result
    
    def run_all_tests(self) -> Dict:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ HR RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        print("=" * 80)
        
        total_tests = len(self.test_cases)
        passed_tests = 0
        failed_tests = 0
        error_tests = 0
        total_time = 0
        total_quality_score = 0
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        
        for i, test_case in enumerate(self.test_cases, 1):
            print(f"\n[{i:2d}/{total_tests}] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
            
            result = self.run_single_test(test_case)
            self.results.append(result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            category = test_case["category"]
            if category not in category_stats:
                category_stats[category] = {"total": 0, "passed": 0, "total_score": 0}
            
            category_stats[category]["total"] += 1
            category_stats[category]["total_score"] += result["quality_metrics"]["overall_score"]
            
            if result["status"] == "PASS":
                passed_tests += 1
                category_stats[category]["passed"] += 1
            elif result["status"] == "FAIL":
                failed_tests += 1
            else:
                error_tests += 1
            
            total_time += result["response_time_ms"]
            total_quality_score += result["quality_metrics"]["overall_score"]
        
        # ê²°ê³¼ ìš”ì•½
        success_rate = (passed_tests / total_tests) * 100
        avg_response_time = total_time / total_tests
        avg_quality_score = total_quality_score / total_tests
        
        summary = {
            "total_tests": total_tests,
            "passed": passed_tests,
            "failed": failed_tests,
            "errors": error_tests,
            "success_rate": round(success_rate, 2),
            "avg_response_time_ms": round(avg_response_time, 2),
            "avg_quality_score": round(avg_quality_score, 2),
            "category_stats": category_stats
        }
        
        self.print_summary(summary)
        return summary
    
    def print_summary(self, summary: Dict):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š HR RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        
        print(f"ì „ì²´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}ê°œ")
        print(f"ì„±ê³µ: {summary['passed']}ê°œ")
        print(f"ì‹¤íŒ¨: {summary['failed']}ê°œ")
        print(f"ì˜¤ë¥˜: {summary['errors']}ê°œ")
        print(f"ì„±ê³µë¥ : {summary['success_rate']}%")
        print(f"í‰ê·  ì‘ë‹µì‹œê°„: {summary['avg_response_time_ms']}ms")
        print(f"í‰ê·  í’ˆì§ˆì ìˆ˜: {summary['avg_quality_score']}/100")
        
        # ì„±ëŠ¥ í‰ê°€
        if summary["avg_response_time_ms"] < 3000:
            print("âš¡ ì‘ë‹µì†ë„: ìš°ìˆ˜ (3ì´ˆ ë¯¸ë§Œ)")
        elif summary["avg_response_time_ms"] < 5000:
            print("ğŸ‘ ì‘ë‹µì†ë„: ì–‘í˜¸ (5ì´ˆ ë¯¸ë§Œ)")
        else:
            print("âš ï¸ ì‘ë‹µì†ë„: ê°œì„  í•„ìš” (5ì´ˆ ì´ˆê³¼)")
        
        if summary["avg_quality_score"] >= 80:
            print("ğŸ¯ ì‘ë‹µí’ˆì§ˆ: ìš°ìˆ˜")
        elif summary["avg_quality_score"] >= 60:
            print("ğŸ‘ ì‘ë‹µí’ˆì§ˆ: ì–‘í˜¸")
        else:
            print("âš ï¸ ì‘ë‹µí’ˆì§ˆ: ê°œì„  í•„ìš”")
        
        print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
        for category, stats in summary["category_stats"].items():
            if stats["total"] > 0:
                rate = (stats["passed"] / stats["total"]) * 100
                avg_score = stats["total_score"] / stats["total"]
                print(f"  {category}: {stats['passed']}/{stats['total']} ({rate:.1f}%, í’ˆì§ˆ: {avg_score:.1f})")
        
        # ì „ì²´ í‰ê°€
        if summary["success_rate"] >= 90 and summary["avg_quality_score"] >= 75:
            print(f"\nğŸ‰ ìš°ìˆ˜í•¨: RAG ì‹œìŠ¤í…œì´ ê³ í’ˆì§ˆ ì‘ë‹µ ìƒì„±")
        elif summary["success_rate"] >= 80 and summary["avg_quality_score"] >= 60:
            print(f"\nğŸ‘ ì–‘í˜¸í•¨: ëŒ€ë¶€ë¶„ ì ì ˆí•œ ì‘ë‹µ, ì¼ë¶€ ê°œì„  í•„ìš”")
        elif summary["success_rate"] >= 70:
            print(f"\nâš ï¸ ì£¼ì˜: ë²¡í„° ê²€ìƒ‰ ë˜ëŠ” ë¦¬ë­ì»¤ íŠœë‹ í•„ìš”")
        else:
            print(f"\nğŸš¨ ì‹¬ê°: RAG ì‹œìŠ¤í…œ ì „ë°˜ì ì¸ ì ê²€ í•„ìš”")
    
    def save_results(self, filename: str = "hr_rag_test_results.json"):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_type": "HR RAG System Test",
            "results": self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = HRRAGTester()
    
    try:
        summary = tester.run_all_tests()
        tester.save_results()
        
        # ì„±ê³µë¥ ì— ë”°ë¥¸ ì¢…ë£Œ ì½”ë“œ ì„¤ì •
        if summary["success_rate"] >= 80:
            exit_code = 0
        else:
            exit_code = 1
            
        print(f"\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ. ì¢…ë£Œ ì½”ë“œ: {exit_code}")
        return exit_code
        
    except Exception as e:
        print(f"\nğŸš¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main()) 